[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at Purdue University\u0026rsquo;s Innovation Science Lab. My work focuses on better understanding the tension between risk management and growth in enterprises with the objective of building resilient organizations. Specific research interests include understanding enterprise risks as complex networks and pursuing systematic, repeatible innovation using an innovation toolkit.\n A co-authored proposal to study enterprise risk management employing complexity theory is currently in review at the National Science Foundation.   Please email me at sheth7@purdue.edu\n","date":1597536000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1597536000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ananya-sheth/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ananya-sheth/","section":"authors","summary":"I am a PhD candidate at Purdue University\u0026rsquo;s Innovation Science Lab. My work focuses on better understanding the tension between risk management and growth in enterprises with the objective of building resilient organizations.","tags":null,"title":"Ananya Sheth","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Joseph V. Sinfield","Ananya Sheth","Romika R. Kotian"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597536000,"objectID":"812b7afa333a9774561777a431cd6702","permalink":"/publication/sftr2020/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/publication/sftr2020/","section":"publication","summary":"Using Comprehensive Success Factor Analysis to Frame Socio-technical Grand Challenges","tags":null,"title":"Framing the Intractable -- Comprehensive Success Factor Analysis for Grand Challenges","type":"publication"},{"authors":["Ananya Sheth","Allyson Ettinger","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"30005601a62dc2e6b488af9042bce5df","permalink":"/publication/bertpriming/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/bertpriming/","section":"publication","summary":"Using semantic priming to investigate how BERT utilizes lexical relations to inform word probabilities in context.","tags":null,"title":"Exploring Lexical Relations in BERT using Semantic Priming (Forthcoming)","type":"publication"},{"authors":["Ananya Sheth","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1592265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592265600,"objectID":"aacb1e6c1a7d5746255fe7c606003e18","permalink":"/publication/nafips20/","publishdate":"2020-06-16T00:00:00Z","relpermalink":"/publication/nafips20/","section":"publication","summary":"Analyzing BERT's word prediction in context from the lens of Ontological Semantics","tags":null,"title":"An Approximate Perspective on Word Prediction in Context: Ontological Semantics meets BERT (Forthcoming)","type":"publication"},{"authors":["Qiaofei Ye","Ananya Sheth","Hemanth Devarapalli","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"1bc1d73455970cb97ad45d42f6293631","permalink":"/publication/nonfactoid/","publishdate":"2019-11-28T00:00:00Z","relpermalink":"/publication/nonfactoid/","section":"publication","summary":"Adding sentiment based features to non-factoid question answering","tags":null,"title":"A Sentiment Based Non-Factoid Question-Answering Framework","type":"publication"},{"authors":["Ananya Sheth","Hemanth Devarapalli","Tatiana Ringenberg","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"522226664ce0f2a00fe61ced53df2e64","permalink":"/publication/aacnn/","publishdate":"2019-11-28T00:00:00Z","relpermalink":"/publication/aacnn/","section":"publication","summary":"Analysing predatory conversations from the lens of authorship attribution.","tags":null,"title":"Authorship Analysis of Online Predatory Conversations using Character Level Convolution Neural Networks","type":"publication"},{"authors":["Tatiana Ringenberg","Ananya Sheth","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"1b4ede5e9d1a860538d322e7361eceb6","permalink":"/publication/fuzzy-risk/","publishdate":"2019-11-28T00:00:00Z","relpermalink":"/publication/fuzzy-risk/","section":"publication","summary":"Formulating online sexual predation risk as a fuzzy set problem.","tags":null,"title":"Not So Cute but Fuzzy: Estimating Risk of Sexual Predation in Online Conversations","type":"publication"},{"authors":["Ananya Sheth","Hemanth Devarapalli","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"4b49f0f882ef91fa489561753b17ea2b","permalink":"/publication/cogsci19/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/cogsci19/","section":"publication","summary":"Using word vectors to investigate the influence of a learner's first language (L1) on semantic errors committed by them in English (L2)","tags":null,"title":"L1 Influence on Content Word errors in Learner English Corpora: Insights from Distributed Representation of Words","type":"publication"},{"authors":["Ananya Sheth","Hemanth Devarapalli","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1563667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563667200,"objectID":"0361473732d54fb470a43ba51672ecf0","permalink":"/publication/iccm/","publishdate":"2019-07-21T00:00:00Z","relpermalink":"/publication/iccm/","section":"publication","summary":"Using word vectors to investigate the influence of a learner's first language (L1) on semantic errors committed by them in English (L2)","tags":null,"title":"Measuring the Influence of L1 on Learner English Errors in Content Words within Word Embedding Models","type":"publication"},{"authors":null,"categories":["neural-networks"],"content":" Summary In this post, I want to briefly demonstrate how one can implement a simple Neural Network architecture in R using a new package for array/tensor computation called rray along with the R6 object oriented programming system which will be used to modularize the various elements requried for anyone to build a neural network architecture. Using our neural network toolkit, I will demonstrate the quintessential example where Linear Models fail but Neural Networks succeed: the XOR function. Most of the code is going to mimick Joel Grus’ awesome code for “Building a Deep Learning Library” resource found here, I cannot state enough about how helpful his videos and blog posts have been. An important difference is that I have ported his python code to R and used rray as the tensor computation package to do all the heavylifting.\n Neural Network Primer (an inadequate one) Specifically, the post covers implementing multilayer perceptrons (MLP). MLPs are prototypical deep learning models whose goal is to approximate a function, \\(\\mathbf{y} = f(\\mathbf{x}|\\Theta)\\), where \\(\\mathbf{x}\\) are the inputs and \\(\\mathbf{y}\\) are the outputs and it does so by learning the best parameters captured in the collection \\(\\Theta\\). These are also referred to as Feed-forward Neural Networks because information flows only in one direction: forward through a series of functions to compute the output values1. The simplest form of a multi-layer perceptron is composed of a series of transformations to the input:\n\\[ f_1 = xW_1 + b_1 \\\\ g = \\phi(f_1(x)) \\\\ f_2 = gW_2 + b_2 \\] Combining these equations into one,\n\\[ MLP(x) = \\phi(xW_1 + b_1)W_2 + b_2 \\]\nHere \\(f_1\\) is a single perceptron, which is a linear transformation of the input vector \\(x\\), using the matrix \\(W\\). \\(\\phi\\) is a non-linear function that is often referred to as an activation function that (in this case) transforms the input into something from which a simple linear model, \\(f_2\\) can be easily trained to accurately predict the correct output. The weight matrices \\(\\{W_1, W_2\\}\\) and the bias vectors \\(\\{b_1, b_2\\}\\) form our parameters which are learnt during training.\nHow is the network trained and how are the parameters learnt? Using the combination of loss functions and gradient descent methods of course! More precisely, in a typical usecase of Neural Networks, one would have inputs (\\(\\mathbf{x}\\)) and the corresponding outputs (\\(\\mathbf{y}\\)) that the network has to be trained to predict, loss functions operationalize this by assigning a numerical score to the produced output of a neural network (denoted by \\(\\mathbf{\\hat{y}}\\)) by comparing it to the desired output using some mathematical function(s), i.e., they try to quantify how bad are the predictions of the neural network. Once the score is calculated, it is important for the model to tweak the gradients such that this score is minimized. This is done using Gradient based learning methods. I do not want to dwelve deeper into theory, but in brief, gradient based methods try to minimize this loss score in an iterative process by:\nComputing the loss estimate Computing the rate change of the loss (called gradients or derivatives) with respect to the parameters Shifting the parameter values in a direction opposite to that of the gradients.  In our instance, we will be looking at the canonical example of Gradient based methods, the Stochastic Gradient Descent (SGD).\nThis is not the ideal introduction to Neural Networks and some the resources that do an infinitely better job than I did are the deep learning book by Goodfellow et al. (2016) and Stanford’s CS231n class among others.\n Computing with Vectors (tensors?) in R: a rray of hope Since in most cases the inputs and outputs that are used in the training of neural networks are often vectors, we will be using a package in R that provides useful tools to perform computation with these vectors. rray is a new package in R made by Davis Vaughn with many different goals, but the major one being: it enables seamless integration of array/tensor broadcasting across all widely used functions including ones that could not have been done (easily) by using the Matrix package (which is extremely awesome and fast!). An example can be shown here:\nLets say you have an \\(3 \\times 2\\) matrix, and you want to center it columnwise. With rray, it is a simple operation, like so:\nlibrary(rray) set.seed(1234) x \u0026lt;- rray(rnorm(6), dim = c(3,2)) x - rray_mean(x, axes = 1) ## \u0026lt;rray\u0026lt;dbl\u0026gt;[,2][3]\u0026gt; ## [,1] [,2] ## [1,] -1.2586673 -1.8755253 ## [2,] 0.2258277 0.8992971 ## [3,] 1.0328396 0.9762283 In contrast, the default matrix in R is not able to do this properly:\nset.seed(1234) x_mat \u0026lt;- matrix(rnorm(6), nrow = 3) x_mat - colMeans(x_mat) ## [,1] [,2] ## [1,] -1.2586673 -1.8755253 ## [2,] 0.7476016 0.3775231 ## [3,] 1.0328396 0.9762283 and with the Matrix library, it becomes a little ugly:\nlibrary(Matrix) set.seed(1234) x_Mat \u0026lt;- Matrix(rnorm(6), nrow = 3) t(t(x_Mat) - Matrix(Matrix::colMeans(x_Mat))) ## 3 x 2 Matrix of class \u0026quot;dgeMatrix\u0026quot; ## [,1] [,2] ## [1,] -1.2586673 -1.8755253 ## [2,] 0.2258277 0.8992971 ## [3,] 1.0328396 0.9762283 Hopefully, the point is clear, but for more interesting utilities of broadcasting, check out this.\n Yet another R OOP system: R6 Generally, a Neural Network model will be made up of several components. We need to keep track of the inputs, the initialization of weights, the logic of the forward pass, the gradients with respect to the loss function (usually done using auto-diff libraries, but we will be hand coding them), and finally the update rule (gradient based methods in our case). The model could be made up using many different kinds of layers and activation functions, thus in order for us to construct such a model efficiently, using Object Oriented Programming proves to be very useful. The major advantage it gives us is Encapsulation, which helps us abstract the data (parameters of the network) as well as functions (forward, and in our case, backward pass logic) together into an object; and Polymorphism, which allows us to use the same method, say the forward pass for any kind of a model.\nR provides us with several options for using Object Oriented Paradigms: S3, S4 and R6, but we will be using R6 since it facilitates encapsulation more easily than the others. For a detailed discussion on the trade-offs between the OOP systems in R, I refer the reader to Chapter 16 of Hadley Wickham’s awesome second edition of Advanced R.\n Neural Network Layers Layers are fundamental structural elements of the network architecture, each layer is usually a function that maps a vector to another vector (or a scalar in certail cases). A typical NN is just a stack of these layers that start from taking in the input data, performing their respective functions, and producing the output.\nTo start off, lets load the required libraries.\nlibrary(rray) library(R6) set.seed(1234) Then, we’d want to describe a general structure of a layer. We do this by defining an R6 class called Layer. We will need two public fields, the parameters of the layer, and the gradients to pass backward during the backpropagation step. Each layer will also contain two functions: (1) logic about the forward computation for moving ahead in the network, and (2) the backward computation, for computing and updating the gradients with respect to the input and the parameters.\nLayer \u0026lt;- R6Class( \u0026quot;Layer\u0026quot;, public = list( params = list(), grads = list(), initialize = function() {}, forward = function() {stop(\u0026quot;Not Implemented!\u0026quot;)}, backward = function() {stop(\u0026quot;Not Implemented!\u0026quot;)} ) ) Since this is a generic layer method, we will not implement anything here, but define a new type of layer by extending this class. Lets define a Linear layer, containing a weights matrix, \\(\\mathbf{W}\\) and a bias vector, \\(\\mathbf{b}\\). The forward computation, given input \\(\\mathbf{x}\\) is simply:\n\\[ \\mathbf{x \\cdot W + b} \\]\nTo compute the gradients\nLinear \u0026lt;- R6Class( \u0026quot;Linear\u0026quot;, inherit = Layer, public = list( inputs = NULL, initialize = function(input_size, output_size) { # super$initialize() self$params[[\u0026quot;w\u0026quot;]] = rray(rnorm(input_size * output_size), c(input_size, output_size)) self$params[[\u0026quot;bias\u0026quot;]] = rray(rnorm(output_size), c(1, output_size)) }, forward = function(inputs) { self$inputs = inputs return(self$inputs %*% self$params[[\u0026quot;w\u0026quot;]] + self$params[[\u0026quot;bias\u0026quot;]]) }, backward = function(grad) { self$grads[[\u0026quot;w\u0026quot;]] = t(self$inputs) %*% grad self$grads[[\u0026quot;bias\u0026quot;]] = rray_sum(grad, axes = 1) return(grad %*% t(self$params[[\u0026quot;w\u0026quot;]])) } ) )   there are other types of networks where this isn’t the case, such as Recurrent Neural Networks (RNN) where the output of a network is used as an input to another network↩\n   ","date":1560902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560902400,"objectID":"e0cae4d057868600bc2469315ae0c6ce","permalink":"/post/implementing-a-neural-network-from-scratch-using-rrays/","publishdate":"2019-06-19T00:00:00Z","relpermalink":"/post/implementing-a-neural-network-from-scratch-using-rrays/","section":"post","summary":"Summary In this post, I want to briefly demonstrate how one can implement a simple Neural Network architecture in R using a new package for array/tensor computation called rray along with the R6 object oriented programming system which will be used to modularize the various elements requried for anyone to build a neural network architecture.","tags":["R"],"title":"Implementing a Neural Network from scratch in R using rrays","type":"post"},{"authors":["Tatiana Ringenberg","Ananya Sheth","Kathryn C. Seigfried-Spellar","Julia Taylor Rayz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1553731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553731200,"objectID":"9ef0d4994734d2b1b2df1e4f3023fa8a","permalink":"/publication/irc/","publishdate":"2019-03-28T00:00:00Z","relpermalink":"/publication/irc/","section":"publication","summary":"Exploring features to classify online sexual predators.","tags":null,"title":"Exploring Automatic Identification of Fantasy-Driven and Contact-Driven Sexual Solicitors","type":"publication"},{"authors":["Kathryn C. Seigfried-Spellar","Marcus K. Rogers","Julia Taylor Rayz","Shih-Feng Yang","Ananya Sheth","Tatiana Ringenberg"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1550880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550880000,"objectID":"98831a8cb95747a6236f25eeb3a2e414","permalink":"/publication/catt/","publishdate":"2019-02-23T00:00:00Z","relpermalink":"/publication/catt/","section":"publication","summary":"We develop a forensically sound investigative tool that, based on natural language processing methods, analyzes and compares chats between minors and contact-driven vs. non-contract driven offenders.","tags":null,"title":"Chat Analysis Triage Tool: Differentiating contact-driven vs. fantasy-driven child sex offenders","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":" Background During the last semester of my undergraduate education at Purdue, I was engaged in a research project that analyzed conversation between two participants, and delivered some insight regarding the two participants’ future interaction(this will be expanded further in a blog post maybe). This problem somewhat involved authorship profiling and verification, two fields that have been heavily studied in traditional NLP problems and along with authorship attribution are collectively known as Stylometry. Stylometry assumes that each author has a specific style that he/she employs in her writing and uses statistical and computational methods in order to either profile the author, determine the author or verify the author of a given written material.\nThe Federalist Papers problem is a classic stylometry problem that was first studied using statistical toolkits by Mostellar and Wallace. The papers were written as essays between 1787 - 1788 by Alexander Hamilton, John Jay and James Madison (who later became the president of United States) to promote the ratification of the constitution. They were all authored under the pseudonym ‘Publius’, which was a tribute to the founder of the Roman Republic, but were then confirmed to be written by the three authors where Hamilton wrote 51 essays, Jay wrote 5, Madison wrote 14, and Hamilton and Madison co-authored 3. The authorship of the remaining 12 has been in dispute. Mostellar and Wallace used probability distributions to represent word frequencies and concluded that the 12 papers with disputed authorship were written by Madison.\nIn this post, I will leverage some of the open source contributions I made to the R packages widyr and tidytext and combine them to present a naive analysis of the authorship of the disputed papers.\n Contributing code to R packages Before I move on, I would like to thank the creators of the widyr and tidytext packages, Julia Silge (for tidytext) and David Robinson (for widyr and tidytext) to have given me the chance to add new features to their packages.\nWidyr This was my first ever code contributing open source contribution, where I added the pairwise_delta method to a list of pairwise calculations that widyr offers. This method essentially implements the Burrows’ Delta method which is a distance calculation between documents and has stylometric benefits. It can be mathematically defined as:\n\\[ \\frac{1}{n}\\sum_{i = 1}^{n}{|z_i(D_1) - z_i(D_2)|} \\]\nOr, for 2 documents \\(D_1\\) and \\(D_2\\) the average manhattan distance between the z-scores for word frequencies of word \\(i\\) in the documents. The z-scores standardize the frequencies of each word to have 0 mean and 1 standard deviation (normal distribution centered around 0). There has been a little bit of dispute about the mathematical foundations of this method, the explanation and resolution of which can be found in Argamon’s paper, but since it has historically worked so well in authorship attribution, it is still used when distance based methods get employed.\n Tidytext I’d personally describe this countribution as ‘cheeky’ because I basically added very few lines of code but that is just because how well the foundations of adding new material to the package’s function is. I implemented the functionality of tokenizing by character ngrams, also called as character_shingles.\nA character shingle is basically a contiguous sequence of characters from a given piece of text. Something like:\n Where we can see how a character 5-gram is constructed (this example uses spaces, but we will be ignoring any punctuation to keep things simple).\nCharacter ngrams work well in certain nlp tasks as features of a document feature matrix, because they can:\nReduce the number of features. Capture cutural morphological differences of the same word (color and colour would be captured as col, etc. when n = 3). Detect misspellings.  Thus, we can, in theory, leverage character shingles as our features in hopes of detecting styles in our authorship problem.\n  Loading libraries We can get all the federalist papers corpus from the corpus library.\nlibrary(corpus) library(tidyverse) library(widyr) library(tidytext) federalist \u0026lt;- as.tibble(federalist) We can quickly glance the number of papers per author\nfederalist %\u0026gt;% count(author) ## # A tibble: 4 x 2 ## author n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Hamilton 51 ## 2 Jay 5 ## 3 Madison 14 ## 4 \u0026lt;NA\u0026gt; 15 The 15 NAs include the ones co-authored by Hamilton and Madison, these are Nos. 18-20. We remove them since we cannot determine which parts of the papers were written by which author. We also remove the ones written by Jay since the disputed papers are believed to be written by either Hamilton or Madison.\nfed_papers \u0026lt;- federalist %\u0026gt;% replace_na(list(author = \u0026quot;Unknown\u0026quot;)) %\u0026gt;% filter(!(name %in% paste(\u0026quot;Federalist No.\u0026quot;, as.character(18:20))), author != \u0026quot;Jay\u0026quot;) fed_papers ## # A tibble: 77 x 6 ## name title venue date author text ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Federalist No. 1 Gener… For … NA Hamil… \u0026quot;To the People of the… ## 2 Federalist No. 6 Conce… For … NA Hamil… \u0026quot;To the People of the… ## 3 Federalist No. 7 The S… For … NA Hamil… \u0026quot;To the People of the… ## 4 Federalist No. 8 The C… From… 1787-11-20 Hamil… \u0026quot;To the People of the… ## 5 Federalist No. 9 The U… For … NA Hamil… \u0026quot;To the People of the… ## 6 Federalist No. 10 The S… From… 1787-11-23 Madis… \u0026quot;To the People of the… ## 7 Federalist No. 11 The U… For … NA Hamil… \u0026quot;To the People of the… ## 8 Federalist No. 12 The U… From… 1787-11-27 Hamil… \u0026quot;To the People of the… ## 9 Federalist No. 13 Advan… For … NA Hamil… \u0026quot;To the People of the… ## 10 Federalist No. 14 Objec… From… 1787-11-30 Madis… \u0026quot;To the People of the… ## # ... with 67 more rows Now that we have content written by 3 authors - Hamilton, Madison and ‘Unknown’, we can compare the styles of each author by calculating the delta metric using my pairwise_delta implementation. Specifically, we can calculate the delta distance by considering relative frequencies of character ngrams/shingles that are evaluated by the 'character_shingles' argument passed to the unnest_tokens method in tidytext, which by default makes character trigrams.\n# Make an author-paper mapping that can be used later. fed_authors \u0026lt;- fed_papers %\u0026gt;% select(name, author) fed_shingles \u0026lt;- fed_papers %\u0026gt;% select(name, text) %\u0026gt;% group_by(name) %\u0026gt;% unnest_tokens(shingle, text, \u0026quot;character_shingles\u0026quot;) %\u0026gt;% ungroup() fed_shingles %\u0026gt;% count(shingle) %\u0026gt;% arrange(-n) ## # A tibble: 6,067 x 2 ## shingle n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 the 21697 ## 2 ion 7321 ## 3 tio 5976 ## 4 ent 5388 ## 5 oft 5139 ## 6 and 5060 ## 7 fth 5060 ## 8 ati 3956 ## 9 nth 3879 ## 10 tha 3633 ## # ... with 6,057 more rows There are over 6000 different character trigrams in the whole corpus, but we don’t have to consider all the trigrams as features. Burrows’ Delta was defined to include the n most frequent words (since it was defined only for words), so we can include the n most frequent features, or trigrams in our analysis. Let’s pick an arbritrary number, say 1000 (if this was a research paper, we would have evaluated the proper number of features by looking at maybe the clustering quality by cliustering on the delta and choosing n where the rand index is maximum)\ntop_shingles \u0026lt;- fed_shingles %\u0026gt;% count(shingle) %\u0026gt;% top_n(1000, n) top_shingles ## # A tibble: 1,004 x 2 ## shingle n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 abl 1099 ## 2 acc 239 ## 3 ace 395 ## 4 ach 483 ## 5 aco 374 ## 6 act 820 ## 7 ade 428 ## 8 adi 234 ## 9 adm 290 ## 10 adv 294 ## # ... with 994 more rows We can now filter all our documents in fed_shingles to have only the trigrams that are in the top 1000 trigrams of the entire corpus while simultaneously also computing the relative frequencies of the trigrams (do this prior to filtering).\nfed_freq \u0026lt;- fed_shingles %\u0026gt;% count(name, shingle) %\u0026gt;% group_by(name) %\u0026gt;% mutate(rel_freq = n/sum(n)) %\u0026gt;% ungroup() %\u0026gt;% filter(shingle %in% top_shingles$shingle) fed_freq ## # A tibble: 73,910 x 4 ## name shingle n rel_freq ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Federalist No. 1 abl 9 0.00117 ## 2 Federalist No. 1 acc 2 0.000260 ## 3 Federalist No. 1 ace 2 0.000260 ## 4 Federalist No. 1 ach 1 0.000130 ## 5 Federalist No. 1 act 5 0.000651 ## 6 Federalist No. 1 ade 4 0.000520 ## 7 Federalist No. 1 adi 3 0.000390 ## 8 Federalist No. 1 adm 2 0.000260 ## 9 Federalist No. 1 adv 3 0.000390 ## 10 Federalist No. 1 aff 3 0.000390 ## # ... with 73,900 more rows The pairwise family of functions in widyr need 3 things as inputs: the item/document column where each value denotes an individual item which is repeated to account for each feature represented by a feature column (in long format as opposed to wide), and the values of the feautures corresponding to the document, once this is passed, the following workflow takes place:\n Widyr essentially takes a long format data, widens it to something you normally see, a matrix format, performs the pairswise operation to return a pairwise matrix, and re-formats it into a long format to give item-item pairwise long tibble with the respective pairwise metric values.\nfed_deltas \u0026lt;- fed_freq %\u0026gt;% pairwise_delta(name, shingle, rel_freq) fed_deltas ## # A tibble: 5,852 x 3 ## item1 item2 delta ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Federalist No. 10 Federalist No. 1 1.02 ## 2 Federalist No. 11 Federalist No. 1 1.07 ## 3 Federalist No. 12 Federalist No. 1 1.04 ## 4 Federalist No. 13 Federalist No. 1 1.18 ## 5 Federalist No. 14 Federalist No. 1 1.00 ## 6 Federalist No. 15 Federalist No. 1 0.948 ## 7 Federalist No. 16 Federalist No. 1 1.06 ## 8 Federalist No. 17 Federalist No. 1 1.07 ## 9 Federalist No. 21 Federalist No. 1 1.03 ## 10 Federalist No. 22 Federalist No. 1 0.902 ## # ... with 5,842 more rows We now have each document, and its measure of naive stylistic similarity(or deviance) with respect to every other document, we can use this to analyse the authorship of the 12 disputed papers.\n Reaching higher dimensions Since Delta is a distance measure, the ones with lower values are close to each other, while ones with larger values are less similar. We can effectively visualize this using a multi-dimensional scaling method which takes a complete pairwise distance matrix and defines coordinates for each individual document (or item) such that the distance between every document with every other document is more or less maintained (there is some information loss).\nMDS exists for base R but hasn’t been implemented for something like a widyr-processed tibble, to make this work, I implemented it so that it can become friendly with widyr based outputs, with the following code, you can see how a widyr function can be constructed!\nmulti_scale \u0026lt;- function(tbl, item1, item2, value, k = 2) { multi_scale_(tbl, widyr:::col_name(substitute(item1)), widyr:::col_name(substitute(item2)), widyr:::col_name(substitute(value)), k = 2) } multi_scale_ \u0026lt;- function(tbl, item1, item2, value, k = 2) { tbl_matrix \u0026lt;- tbl %\u0026gt;% spread(item2, widyr:::col_name(value), fill = 0) %\u0026gt;% as.data.frame() %\u0026gt;% remove_rownames() %\u0026gt;% column_to_rownames(\u0026quot;item1\u0026quot;) %\u0026gt;% as.matrix() cmdscale(tbl_matrix, k = k) %\u0026gt;% as.data.frame() %\u0026gt;% rownames_to_column(\u0026quot;item\u0026quot;) %\u0026gt;% as.tibble() } We can now simply pass the item-item pairwise delta tibble to multi_scale to return something that can easily work with ggplot2 to understand our results better:\nfed_deltas %\u0026gt;% multi_scale(item1, item2, delta) %\u0026gt;% ggplot(aes(V1, V2)) + geom_point() This is great, but we surely need to represent each document by its author, and so we can add a color aesthetic by joining the multiscaled data to the author-paper mapping we created earlier.\nfed_deltas %\u0026gt;% multi_scale(item1, item2, delta) %\u0026gt;% inner_join(fed_authors, by = c(item = \u0026quot;name\u0026quot;)) %\u0026gt;% ggplot(aes(V1, V2, color = author)) + geom_point(size = 2, alpha = 0.7) + scale_y_continuous(limits = c(-0.7, 0.7), breaks = scales::pretty_breaks(10)) + scale_x_continuous(limits = c(-0.7, 0.7), breaks = scales::pretty_breaks(10)) + scale_color_manual(values = c(\u0026quot;#f26d5b\u0026quot;, \u0026quot;#FFBC42\u0026quot;, \u0026quot;#2b90d9\u0026quot;)) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), legend.position = \u0026quot;top\u0026quot;, panel.grid.minor = element_blank(), legend.text = element_text(size = rel(1)) ) + labs( title = \u0026quot;Authorship Analysis of the Federalist Papers\u0026quot;, y = \u0026quot;Dimension 2\u0026quot;, x = \u0026quot;Dimension 2\u0026quot;, color = \u0026quot;\u0026quot;, subtitle = \u0026quot;Papers with disputed authors lie far apart from Hamilton\\nbut much closer to Madison\u0026quot; ) This plot shows what I described earlier, a 2-dimension representation of the documents having the deviation from each other more or less maintained, accompanied by a little information loss. The dimensions don’t mean much and are arbritrarily defined, unlike PCA where you can study the contribution of each feature to the PCs. But what we see pretty much supports the conclusion by Mostellar and Wallace that the 12 papers with unknown authorship are far away from papers written by Hamilton but are closer to the papers authored by Madison.\nIn this post, I quickly demonstrated a naive analysis of the federalist papers problem using my open source contributions along with some very useful tools provided by tidytext, widyr and the tidyverse suite of packages. I enjoyed contributing to open source very much and hope to continue to do so now that I have the opportunity to learn more about Natural Language Processing as I venture into rigorous research as I begin my PhD studies at Purdue starting this fall. Please let me know if you’d like to know more about the work done in this blog post or anything else or if you have any feedback!\n ","date":1527120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527120000,"objectID":"d8480ce322c05db54ada163e4985ab72","permalink":"/post/my-first-few-open-source-contributions-authorship-attribution-of-the-federalist-papers/","publishdate":"2018-05-24T00:00:00Z","relpermalink":"/post/my-first-few-open-source-contributions-authorship-attribution-of-the-federalist-papers/","section":"post","summary":"Background During the last semester of my undergraduate education at Purdue, I was engaged in a research project that analyzed conversation between two participants, and delivered some insight regarding the two participants’ future interaction(this will be expanded further in a blog post maybe).","tags":["stylometry","open source","tidytext"],"title":"My first few open source contributions: Authorship Attribution of the Federalist Papers","type":"post"},{"authors":null,"categories":[],"content":"  Roses are red, violets are blue\n  This is a forced rhyme, here’s blog post two!\n Background Ever since I worked on data about populations at my internship at Perscio, a healthcare data analysis firm in Indianapolis, as well as worked with a Professor of Demography and Social Policy on a paper about demographic data, I have gained interest in population problems - mostly through readings.\nThe best way to restart this journey would be to do so using what population problems often involve: Data analysis. In this post, we define and calculate population growth rates as well as doubling times of several countries and then finally produce intuitive visualizations of these numbers.\n Loading libraries and data The data used throughout this post is from United Nations’ Population Divison and consists of population numbers of all countries between 1970 and 2015 (in intervals of 5 years).\nlibrary(tidyverse) library(kani) library(scales) library(geofacet) population_raw \u0026lt;- read_csv(\u0026quot;../../static/data/population.csv\u0026quot;) population_raw ## # A tibble: 273 x 68 ## Country code `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957` ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 WORLD 900 2 536… 2 583… 2 630… 2 677… 2 724… 2 772… 2 821… 2 871… ## 2 More dev… 901 814 8… 824 2… 834 0… 844 2… 854 6… 865 0… 875 5… 885 9… ## 3 Less dev… 902 1 721… 1 759… 1 796… 1 832… 1 869… 1 907… 1 945… 1 986… ## 4 Least de… 941 195 2… 199 0… 202 9… 206 8… 211 0… 215 4… 220 0… 224 8… ## 5 Less dev… 934 1 526… 1 560… 1 593… 1 626… 1 658… 1 691… 1 725… 1 761… ## 6 Less dev… 948 1 157… 1 179… 1 203… 1 229… 1 256… 1 284… 1 314… 1 344… ## 7 High-inc… 1503 672 8… 680 6… 688 8… 697 4… 706 2… 715 2… 724 3… 733 4… ## 8 Middle-i… 1517 1 734… 1 772… 1 808… 1 844… 1 880… 1 916… 1 954… 1 992… ## 9 Upper-mi… 1502 956 2… 980 1… 1 001… 1 022… 1 041… 1 060… 1 079… 1 099… ## 10 Lower-mi… 1501 778 2… 792 1… 807 0… 822 6… 839 1… 856 4… 874 4… 893 2… ## # ... with 263 more rows, and 58 more variables: `1958` \u0026lt;chr\u0026gt;, ## # `1959` \u0026lt;chr\u0026gt;, `1960` \u0026lt;chr\u0026gt;, `1961` \u0026lt;chr\u0026gt;, `1962` \u0026lt;chr\u0026gt;, `1963` \u0026lt;chr\u0026gt;, ## # `1964` \u0026lt;chr\u0026gt;, `1965` \u0026lt;chr\u0026gt;, `1966` \u0026lt;chr\u0026gt;, `1967` \u0026lt;chr\u0026gt;, `1968` \u0026lt;chr\u0026gt;, ## # `1969` \u0026lt;chr\u0026gt;, `1970` \u0026lt;chr\u0026gt;, `1971` \u0026lt;chr\u0026gt;, `1972` \u0026lt;chr\u0026gt;, `1973` \u0026lt;chr\u0026gt;, ## # `1974` \u0026lt;chr\u0026gt;, `1975` \u0026lt;chr\u0026gt;, `1976` \u0026lt;chr\u0026gt;, `1977` \u0026lt;chr\u0026gt;, `1978` \u0026lt;chr\u0026gt;, ## # `1979` \u0026lt;chr\u0026gt;, `1980` \u0026lt;chr\u0026gt;, `1981` \u0026lt;chr\u0026gt;, `1982` \u0026lt;chr\u0026gt;, `1983` \u0026lt;chr\u0026gt;, ## # `1984` \u0026lt;chr\u0026gt;, `1985` \u0026lt;chr\u0026gt;, `1986` \u0026lt;chr\u0026gt;, `1987` \u0026lt;chr\u0026gt;, `1988` \u0026lt;chr\u0026gt;, ## # `1989` \u0026lt;chr\u0026gt;, `1990` \u0026lt;chr\u0026gt;, `1991` \u0026lt;chr\u0026gt;, `1992` \u0026lt;chr\u0026gt;, `1993` \u0026lt;chr\u0026gt;, ## # `1994` \u0026lt;chr\u0026gt;, `1995` \u0026lt;chr\u0026gt;, `1996` \u0026lt;chr\u0026gt;, `1997` \u0026lt;chr\u0026gt;, `1998` \u0026lt;chr\u0026gt;, ## # `1999` \u0026lt;chr\u0026gt;, `2000` \u0026lt;chr\u0026gt;, `2001` \u0026lt;chr\u0026gt;, `2002` \u0026lt;chr\u0026gt;, `2003` \u0026lt;chr\u0026gt;, ## # `2004` \u0026lt;chr\u0026gt;, `2005` \u0026lt;chr\u0026gt;, `2006` \u0026lt;chr\u0026gt;, `2007` \u0026lt;chr\u0026gt;, `2008` \u0026lt;chr\u0026gt;, ## # `2009` \u0026lt;chr\u0026gt;, `2010` \u0026lt;chr\u0026gt;, `2011` \u0026lt;chr\u0026gt;, `2012` \u0026lt;chr\u0026gt;, `2013` \u0026lt;chr\u0026gt;, ## # `2014` \u0026lt;chr\u0026gt;, `2015` \u0026lt;chr\u0026gt; The data looks a little weird:\nIt’s in a wide format than a long one, each year seems to be a single column. The population values look to be parsed as characters, this is mostly because I didn’t provide any parsing formats to read_csv()   Tidying data We can all fix this using some of the helper functions in the tidyverse package!\npopulation \u0026lt;- population_raw %\u0026gt;% gather(`1950`:`2015`, key = \u0026quot;year\u0026quot;, value = \u0026quot;population\u0026quot;) %\u0026gt;% mutate( population = as.numeric(str_replace_all(population, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;)), year = as.numeric(year) ) population ## # A tibble: 18,018 x 4 ## Country code year population ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 WORLD 900 1950 2536275 ## 2 More developed regions 901 1950 814865 ## 3 Less developed regions 902 1950 1721410 ## 4 Least developed countries 941 1950 195259 ## 5 Less developed regions, excluding least develop… 934 1950 1526151 ## 6 Less developed regions, excluding China 948 1950 1157197 ## 7 High-income countries 1503 1950 672896 ## 8 Middle-income countries 1517 1950 1734481 ## 9 Upper-middle-income countries 1502 1950 956204 ## 10 Lower-middle-income countries 1501 1950 778277 ## # ... with 18,008 more rows Now that the data is more readable, we can look at what each column describes:\nCountry: Country names (also contains data about regions and world) code: Country code specified by the UN population: Total population of the country in 1000s Year: .. The year  As an example, we can now plot how the population grew for the world, as well as countries with different income situations:\npopulation %\u0026gt;% filter(str_detect(Country, \u0026quot;WORLD|income\u0026quot;)) %\u0026gt;% ggplot(aes(year, population/1000, group = Country, color = Country)) + geom_line(size = 1) + scale_y_continuous(breaks = pretty_breaks(n = 6)) + scale_x_continuous(breaks = pretty_breaks(n = 6)) + scale_color_kani() + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), legend.position = \u0026quot;top\u0026quot;, panel.grid.minor = element_blank(), legend.text = element_text(size = rel(0.8)) ) + labs( title = \u0026quot;Population growth rates in countries\\ndifferentiated by income\u0026quot;, y = \u0026quot;Population per million\u0026quot;, x = \u0026quot;Year\u0026quot;, color = \u0026quot;\u0026quot;, subtitle = \u0026quot;Middle income countries have been experiencing\\nhigher population growth than other countries\u0026quot; )  Population Growth Rate Studying total population numbers is great, but what’s even useful is to look at the rate by which the population changes in regions. The population growth rate of a country can be defined as the rate at which the number of individuals changes over a period of time expressed as a percentage of the population at the beginning of that time period.\nMathematically,\n\\[ Population\\ growth \\ rate = \\frac{Pop(t_2) - Pop(t_1)}{Pop(t_1)(t_2 - t_1)} \\]\nwhere,\n\\(t_1\\) and \\(t_2\\) are beginning and end times of the time period. In our data these are successive years so the difference is always 1.\nand\n\\(Pop(t)\\) is the number of individuals at time \\(t\\).\nWe can use the lag() function in dplyr to calculate the yearly growth rate for each country/region in the dataset. As an example, we can see the population growth rate of the world starting from 1950 as shown in this plot:\npopulation %\u0026gt;% filter(Country == \u0026quot;WORLD\u0026quot;) %\u0026gt;% mutate(growth_rate = population/lag(population, 1) - 1) %\u0026gt;% ggplot(aes(year, growth_rate)) + geom_line(size = 1, color = \u0026quot;#f15c5c\u0026quot;) + scale_y_continuous(labels = percent_format(), limits = c(0, 0.022)) + scale_x_continuous(breaks = pretty_breaks(n = 6)) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), panel.grid.minor = element_blank() ) + labs( x = \u0026quot;Year\u0026quot;, y = \u0026quot;Population Growth Rate (%)\u0026quot;, title = \u0026quot;Population Growth Rate of the World\u0026quot;, subtitle = \u0026quot;Average yearly change in population between 1950-2015\u0026quot; ) But this was for one region in the entire dataset! How can we fit this model for all regions? Easy, we just use map() from the purrr package which lets us extend a function to different kinds of groups within the data which in this case are countries/regions. This can be done by first nesting all the yearly population changes for each country as a dataframe, fitting the desired function for each country, and then unnesting to get rates for all countries.\ngrowth_rate \u0026lt;- function(df) { return(df %\u0026gt;% transmute(growth_rate = population/lag(population, 1) - 1)) } population_growth \u0026lt;- population %\u0026gt;% group_by(Country) %\u0026gt;% nest() %\u0026gt;% mutate(growth = map(data, growth_rate)) %\u0026gt;% unnest() population_growth ## # A tibble: 18,018 x 5 ## Country code year population growth_rate ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 WORLD 900 1950 2536275 NA ## 2 WORLD 900 1951 2583817 0.0187 ## 3 WORLD 900 1952 2630584 0.0181 ## 4 WORLD 900 1953 2677230 0.0177 ## 5 WORLD 900 1954 2724302 0.0176 ## 6 WORLD 900 1955 2772243 0.0176 ## 7 WORLD 900 1956 2821383 0.0177 ## 8 WORLD 900 1957 2871952 0.0179 ## 9 WORLD 900 1958 2924081 0.0182 ## 10 WORLD 900 1959 2977825 0.0184 ## # ... with 18,008 more rows Let’s look at the first plot in this post, but from the perspective of population growth rate:\npopulation_growth %\u0026gt;% filter(str_detect(Country, \u0026quot;WORLD|income\u0026quot;)) %\u0026gt;% ggplot(aes(year, growth_rate, group = Country, color = Country)) + geom_line(size = 1) + scale_y_continuous(breaks = seq(0, 0.03, by = 0.005), limits = c(0, 0.03), labels = percent_format()) + scale_x_continuous(breaks = pretty_breaks(n = 6)) + scale_color_kani() + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), legend.position = \u0026quot;top\u0026quot;, panel.grid.minor = element_blank(), legend.text = element_text(size = rel(0.8)) ) + labs( title = \u0026quot;Population growth rates in countries\\ndifferentiated by income\u0026quot;, y = \u0026quot;Population Growth Rate (%)\u0026quot;, x = \u0026quot;Year\u0026quot;, color = \u0026quot;\u0026quot;, subtitle = \u0026quot;As the world population growth rate falls,\\nlow income countries are experiencing higher growth rates.\u0026quot; ) We see that while the low-income countries line was at the bottom of the chart in the first plot indicating their population numbers havent gone up by much, they still experience the highest percentage changes in their population. Low income counties started at 1.4% growth rate and then jumped up to being the highest in comparison to countries with higher income, 2.7%. This is mostly because of a dual effect: high birth rates and presence of a younger population compared to the rest, but I will most probably explore this further in future posts.\n Doubling Times We now focus on doubling times, or the metric which looks at how long does it take for a region to double its population. This is important because the onset of modernity (starting in mid 20th century), something that brought in better standards of living and health has resulted in a rapid population growth, but that historical growth has now slowed down greatly. The peak growth rate was in 1960s at about 2.1% and has since fallen to about half of that. It would be interesting to see how long it took for the population to double in the 60s versus now.\nMathematically, the doubling time for a given year can be given as follows:\n\\[ Doubling \\ Time = \\frac{\\ln(2)}{r_t} \\]\nWhere \\(r\\) is the growth rate of the region at time \\(t\\). We assume that human population growth follows a exponential curve that explains the \\(\\ln(2)\\) component.\nWe can now use this and fit it to all regions described in the dataset.\npopulation_rates \u0026lt;- population_growth %\u0026gt;% group_by(Country) %\u0026gt;% nest() %\u0026gt;% mutate(doubling_time = map(data, function(df) {return(log(2)/df$growth_rate)})) %\u0026gt;% unnest() population_rates ## # A tibble: 18,018 x 6 ## Country doubling_time code year population growth_rate ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 WORLD NA 900 1950 2536275 NA ## 2 WORLD 37.0 900 1951 2583817 0.0187 ## 3 WORLD 38.3 900 1952 2630584 0.0181 ## 4 WORLD 39.1 900 1953 2677230 0.0177 ## 5 WORLD 39.4 900 1954 2724302 0.0176 ## 6 WORLD 39.4 900 1955 2772243 0.0176 ## 7 WORLD 39.1 900 1956 2821383 0.0177 ## 8 WORLD 38.7 900 1957 2871952 0.0179 ## 9 WORLD 38.2 900 1958 2924081 0.0182 ## 10 WORLD 37.7 900 1959 2977825 0.0184 ## # ... with 18,008 more rows Let’s look at the doubling times of countries based differentiated by income levels as an example:\npopulation_rates %\u0026gt;% filter(str_detect(Country, \u0026quot;WORLD|income\u0026quot;)) %\u0026gt;% ggplot(aes(year, doubling_time, group = Country, color = Country)) + geom_line(size = 1) + scale_y_continuous(breaks = seq(0, 150, by = 25), limits = c(0, 150)) + scale_x_continuous(breaks = pretty_breaks(n = 6)) + scale_color_kani() + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), legend.position = \u0026quot;top\u0026quot;, panel.grid.minor = element_blank(), legend.text = element_text(size = rel(0.8)) ) + labs( title = \u0026quot;Population Doubling times in the world\u0026quot;, subtitle = \u0026quot;Higher income countries take the longest time to double their\\npopulation while the lower income ones take the least time\u0026quot;, y = \u0026quot;Doubling time in years\u0026quot;, x = \u0026quot;Year\u0026quot;, color = \u0026quot;\u0026quot; )  Visualizing Growth and Doubling Times in Different regions So far, we’ve seen growth rates in countries grouped together in bins or buckets based on income levels, what if we wanted to decompose these and actually look at countries? We can always select a bunch of countries and show them in a single graph, or even make separate graphs and show them in the same plot as different boxes using facet_wrap().\nThis is great, but it can also mask regional patterns, what if all Scandinavian countries experienced similar trends? What is an intelligent way to group them together? One way is to manually do it, but this is where the geofacet package comes into play. With the geofacet package, one can create a grid as we will see below and pre-define the positions of each country/region so that they can mimic a world map!\nAs an example, we look at European Countries:\neurope_grid \u0026lt;- data.frame( row = c(1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9), col = c(1, 4, 5, 6, 7, 1, 2, 5, 7, 8, 4, 5, 6, 7, 8, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 6, 7, 8, 4, 6, 7, 8, 7, 8, 6, 7, 8), code = c(\u0026quot;ISL\u0026quot;, \u0026quot;NOR\u0026quot;, \u0026quot;SWE\u0026quot;, \u0026quot;FIN\u0026quot;, \u0026quot;EST\u0026quot;, \u0026quot;IRL\u0026quot;, \u0026quot;GBR\u0026quot;, \u0026quot;DEN\u0026quot;, \u0026quot;LAT\u0026quot;, \u0026quot;RUS\u0026quot;, \u0026quot;NLD\u0026quot;, \u0026quot;DEU\u0026quot;, \u0026quot;POL\u0026quot;, \u0026quot;LTU\u0026quot;, \u0026quot;BLR\u0026quot;, \u0026quot;FRA\u0026quot;, \u0026quot;BEL\u0026quot;, \u0026quot;LUX\u0026quot;, \u0026quot;AUT\u0026quot;, \u0026quot;CZE\u0026quot;, \u0026quot;SVK\u0026quot;, \u0026quot;UKR\u0026quot;, \u0026quot;PRT\u0026quot;, \u0026quot;ESP\u0026quot;, \u0026quot;CHE\u0026quot;, \u0026quot;SVN\u0026quot;, \u0026quot;HUN\u0026quot;, \u0026quot;ROU\u0026quot;, \u0026quot;MDA\u0026quot;, \u0026quot;ITA\u0026quot;, \u0026quot;HRV\u0026quot;, \u0026quot;SRB\u0026quot;, \u0026quot;BGR\u0026quot;, \u0026quot;MNE\u0026quot;, \u0026quot;MKD\u0026quot;, \u0026quot;BIH\u0026quot;, \u0026quot;ALB\u0026quot;, \u0026quot;GRC\u0026quot;), name = c(\u0026quot;Iceland\u0026quot;, \u0026quot;Norway\u0026quot;, \u0026quot;Sweden\u0026quot;, \u0026quot;Finland\u0026quot;, \u0026quot;Estonia\u0026quot;, \u0026quot;Ireland\u0026quot;, \u0026quot;United Kingdom\u0026quot;, \u0026quot;Denmark\u0026quot;, \u0026quot;Latvia\u0026quot;, \u0026quot;Russian Federation\u0026quot;, \u0026quot;Netherlands\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Poland\u0026quot;, \u0026quot;Lithuania\u0026quot;, \u0026quot;Belarus\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Belgium\u0026quot;, \u0026quot;Luxembourg\u0026quot;, \u0026quot;Austria\u0026quot;, \u0026quot;Czechia\u0026quot;, \u0026quot;Slovakia\u0026quot;, \u0026quot;Ukraine\u0026quot;, \u0026quot;Portugal\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Switzerland\u0026quot;, \u0026quot;Slovenia\u0026quot;, \u0026quot;Hungary\u0026quot;, \u0026quot;Romania\u0026quot;, \u0026quot;Republic of Moldova\u0026quot;, \u0026quot;Italy\u0026quot;, \u0026quot;Croatia\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;Bulgaria\u0026quot;, \u0026quot;Montenegro\u0026quot;, \u0026quot;TFYR Macedonia\u0026quot;, \u0026quot;Bosnia and Herzegovina\u0026quot;, \u0026quot;Albania\u0026quot;, \u0026quot;Greece\u0026quot;), stringsAsFactors = FALSE ) euro_facets \u0026lt;- population_rates %\u0026gt;% filter(Country %in% europe_grid$name) %\u0026gt;% ggplot(aes(year, growth_rate, group = Country)) + geom_line(color = \u0026quot;#79bd9a\u0026quot;, size = 1) + scale_y_continuous(labels = percent_format()) + facet_geo(~Country, grid = europe_grid) + theme_kani() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), panel.grid.minor = element_blank(), plot.background = element_rect(fill = \u0026quot;white\u0026quot;), panel.background = element_rect(fill = \u0026quot;white\u0026quot;), strip.background = element_rect(fill = \u0026quot;white\u0026quot;), strip.text.x = element_text(face = \u0026quot;bold\u0026quot;) ) + labs( title = \u0026quot;Population growth rates in Europe\u0026quot;, y = \u0026quot;Population Growth Rate (%)\u0026quot;, x = \u0026quot;\u0026quot;, color = \u0026quot;\u0026quot;, subtitle = \u0026quot;Europe has been facing a bit of a population decline. \u0026quot; ) ggsave(\u0026quot;../../static/img/eu_population_growth.png\u0026quot;, euro_facets, height = 15, width = 20) euro_facets Check enlarged version. We see that most of Europe is beginning to enter the population decline phase, there is a small upward trend in some countries, but this is mostly because of the mass-migration. Most of Europe has already entered the phase of population decline.\nWhat about the doubling times in South America?\nsouth_america_grid \u0026lt;- data.frame( row = c(1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4), col = c(1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4), code = c(\u0026quot;COL\u0026quot;, \u0026quot;VEN\u0026quot;, \u0026quot;GUY\u0026quot;, \u0026quot;SUR\u0026quot;, \u0026quot;GUF\u0026quot;, \u0026quot;ECU\u0026quot;, \u0026quot;PER\u0026quot;, \u0026quot;BOL\u0026quot;, \u0026quot;BRA\u0026quot;, \u0026quot;CHL\u0026quot;, \u0026quot;PRY\u0026quot;, \u0026quot;URY\u0026quot;, \u0026quot;ARG\u0026quot;), name = c(\u0026quot;Colombia\u0026quot;, \u0026quot;Venezuela (Bolivarian Republic of)\u0026quot;, \u0026quot;Guyana\u0026quot;, \u0026quot;Suriname\u0026quot;, \u0026quot;French Guiana\u0026quot;, \u0026quot;Ecuador\u0026quot;, \u0026quot;Peru\u0026quot;, \u0026quot;Bolivia (Plurinational State of)\u0026quot;, \u0026quot;Brazil\u0026quot;, \u0026quot;Chile\u0026quot;, \u0026quot;Paraguay\u0026quot;, \u0026quot;Uruguay\u0026quot;, \u0026quot;Argentina\u0026quot;), stringsAsFactors = FALSE ) sa_facets \u0026lt;- population_rates %\u0026gt;% filter(Country %in% south_america_grid$name) %\u0026gt;% ggplot(aes(year, doubling_time, group = Country)) + geom_line(color = \u0026quot;#8283a7\u0026quot;, size = 1) + scale_x_continuous(breaks = seq(1950, 2010, length = 5)) + scale_y_continuous(breaks = pretty_breaks(7)) + facet_geo(~Country, grid = south_america_grid, scales = \u0026quot;free\u0026quot;) + theme_kani() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text = element_text(size = rel(1)), panel.grid.minor = element_blank(), plot.background = element_rect(fill = \u0026quot;white\u0026quot;), panel.background = element_rect(fill = \u0026quot;white\u0026quot;), strip.background = element_rect(fill = \u0026quot;white\u0026quot;), strip.text.x = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.1)) ) + labs( title = \u0026quot;Population Doubling Times in South America\u0026quot;, x = \u0026quot;\u0026quot;, y = \u0026quot;Doubling Time in years\u0026quot;, color = \u0026quot;\u0026quot;, subtitle = \u0026quot;More stable trend for larger countries, less so for the smaller ones\u0026quot; ) ggsave(\u0026quot;../../static/img/sa_doubling.png\u0026quot;, sa_facets, height = 12, width = 16) sa_facets Check enlarged version. The population boom between 60s and 80s did affect most of South America since there was a decline in the doubling times (for at least the larger countries, by size). This was also a time when the fastest doubling of the world population happened, from 2.5 billion people to 5 billion people in just 37 years (1950 - 1987)! The UN projections with the most likely scenario (SSP2) indicate that by 2088, it will take another 100 years for the world population to double (Our World in Data, 2015).\n Conclusion This was a simple post that introduces some helpful rates and measures to understand population change in the world. The visualizations in the post showed how countries with different income levels (as categorized by the UN) differ in their respective population growth rates as well as doubling times, and then we further decomposed these groupings by plotting the country specific measures using geofacet.\nIt is exciting to see what the future holds in terms of population changes and hope to continue working with more complex demographic data to produce interesting analyses to blog about! I am very happy to get feedback on this post so please reach out to me via Twitter if you have any comments to make!\n ","date":1518566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518566400,"objectID":"fb57c0c2f7ebe259f4628dbedfa422f2","permalink":"/post/population-growth-and-doubling-times-with-tidyverse/","publishdate":"2018-02-14T00:00:00Z","relpermalink":"/post/population-growth-and-doubling-times-with-tidyverse/","section":"post","summary":"Roses are red, violets are blue\n  This is a forced rhyme, here’s blog post two!\n Background Ever since I worked on data about populations at my internship at Perscio, a healthcare data analysis firm in Indianapolis, as well as worked with a Professor of Demography and Social Policy on a paper about demographic data, I have gained interest in population problems - mostly through readings.","tags":["tidyverse","population studies","demographic data"],"title":"Population growth and Doubling times with tidyverse","type":"post"},{"authors":null,"categories":[],"content":" Understanding and accepting mental health as an issue at the workplace has become ever so crucial in recent times. Mental illnesses like depression and anxiety can have a significant economic impact; the estimated cost to the global economy is US$ 1 trillion per year in lost productivity (source). Open Sourcing Mental Illness (OSMI) is a non profit organization that focuses on raising awareness, education and providing resources to support mental wellness at workplaces, especially in the tech industry. In 2014, they conducted their first ever survey which had questions pertaining to how mental health is perceived at tech workplaces by employees and their employers.\nThis survey had over 1200 responses and the data from these responses was made public, which gives us an interesting opportunity to analyze the attitudes of tech-workers from 48 different countries towards mental health.\nLoading libraries and data Let’s load libraries important for this analysis as well as the data which can be downloaded from kaggle: here\nlibrary(tidyverse) library(ebbr) mental_health \u0026lt;- read_csv(\u0026quot;../../static/data/mental-health.csv\u0026quot;) skimr::skim(mental_health) ## Skim summary statistics ## n obs: 1259 ## n variables: 27 ## ## Variable type: character ## variable missing complete n min max empty n_unique ## 1 anonymity 0 1259 1259 2 10 0 3 ## 2 benefits 0 1259 1259 2 10 0 3 ## 3 care_options 0 1259 1259 2 8 0 3 ## 4 comments 1096 163 1259 1 3548 0 159 ## 5 Country 0 1259 1259 5 22 0 48 ## 6 coworkers 0 1259 1259 2 12 0 3 ## 7 family_history 0 1259 1259 2 3 0 2 ## 8 Gender 0 1259 1259 1 46 0 47 ## 9 leave 0 1259 1259 9 18 0 5 ## 10 mental_health_consequence 0 1259 1259 2 5 0 3 ## 11 mental_health_interview 0 1259 1259 2 5 0 3 ## 12 mental_vs_physical 0 1259 1259 2 10 0 3 ## 13 no_employees 0 1259 1259 3 14 0 6 ## 14 obs_consequence 0 1259 1259 2 3 0 2 ## 15 phys_health_consequence 0 1259 1259 2 5 0 3 ## 16 phys_health_interview 0 1259 1259 2 5 0 3 ## 17 remote_work 0 1259 1259 2 3 0 2 ## 18 seek_help 0 1259 1259 2 10 0 3 ## 19 self_employed 18 1241 1259 2 3 0 2 ## 20 state 515 744 1259 2 2 0 45 ## 21 supervisor 0 1259 1259 2 12 0 3 ## 22 tech_company 0 1259 1259 2 3 0 2 ## 23 treatment 0 1259 1259 2 3 0 2 ## 24 wellness_program 0 1259 1259 2 10 0 3 ## 25 work_interfere 264 995 1259 5 9 0 4 ## ## Variable type: numeric ## variable missing complete n mean sd min p25 median p75 max ## 1 Age 0 1259 1259 7.9e+07 2.8e+09 -1726 27 31 36 1e+11 ## hist ## 1 ▇▁▁▁▁▁▁▁ ## ## Variable type: POSIXct ## variable missing complete n min max median n_unique ## 1 Timestamp 0 1259 1259 2014-08-27 2016-02-01 2014-08-28 1246 The skimr package really helps in showing a human-readable, compact summary overview of the data which allows identifying missing values in columns among the other benefits it provides.\nThe data is mostly categorical and in fact includes all responses to questions that correspond to the column names. The column names obviously do not look like questions, but this is because the maintainers of the data have assigned each question a column name and this list can be found here. There are columns with missing data, columns with bizzare values, and columns with values that we can group together (Gender). We can now start our adventure in exploring this data by tidying up these columns.\nmental_health \u0026lt;- mental_health %\u0026gt;% select(-c(state, comments, Timestamp)) %\u0026gt;% filter(between(Age, 18, 90)) %\u0026gt;% mutate( Gender = case_when( str_detect(Gender, regex(\u0026quot;trans|fluid|androgynous\u0026quot;, ignore_case = T)) == T ~ \u0026quot;gender_variance\u0026quot;, str_detect(Gender, regex(\u0026quot;female|femail|f|woman|femake\u0026quot;, ignore_case = T)) == T ~ \u0026quot;female\u0026quot;, str_detect(Gender, regex(\u0026quot;mal*|m|mail|man|guy\u0026quot;, ignore_case = T)) == T ~ \u0026quot;male\u0026quot;, TRUE ~ \u0026quot;gender_variance\u0026quot; ) ) %\u0026gt;% replace_na(list( self_employed = \u0026quot;unknown\u0026quot;, work_interfere = \u0026quot;unknown\u0026quot; )) skimr::skim(mental_health) ## Skim summary statistics ## n obs: 1251 ## n variables: 24 ## ## Variable type: character ## variable missing complete n min max empty n_unique ## 1 anonymity 0 1251 1251 2 10 0 3 ## 2 benefits 0 1251 1251 2 10 0 3 ## 3 care_options 0 1251 1251 2 8 0 3 ## 4 Country 0 1251 1251 5 22 0 46 ## 5 coworkers 0 1251 1251 2 12 0 3 ## 6 family_history 0 1251 1251 2 3 0 2 ## 7 Gender 0 1251 1251 4 15 0 3 ## 8 leave 0 1251 1251 9 18 0 5 ## 9 mental_health_consequence 0 1251 1251 2 5 0 3 ## 10 mental_health_interview 0 1251 1251 2 5 0 3 ## 11 mental_vs_physical 0 1251 1251 2 10 0 3 ## 12 no_employees 0 1251 1251 3 14 0 6 ## 13 obs_consequence 0 1251 1251 2 3 0 2 ## 14 phys_health_consequence 0 1251 1251 2 5 0 3 ## 15 phys_health_interview 0 1251 1251 2 5 0 3 ## 16 remote_work 0 1251 1251 2 3 0 2 ## 17 seek_help 0 1251 1251 2 10 0 3 ## 18 self_employed 0 1251 1251 2 7 0 3 ## 19 supervisor 0 1251 1251 2 12 0 3 ## 20 tech_company 0 1251 1251 2 3 0 2 ## 21 treatment 0 1251 1251 2 3 0 2 ## 22 wellness_program 0 1251 1251 2 10 0 3 ## 23 work_interfere 0 1251 1251 5 9 0 5 ## ## Variable type: numeric ## variable missing complete n mean sd min p25 median p75 max hist ## 1 Age 0 1251 1251 32.08 7.29 18 27 31 36 72 ▂▇▆▂▁▁▁▁ We can ignore a few columns like state, comments and Timestamp since they do not provide much benefit in the analysis. While I really appreciate the survey keeping gender as a free response, employees with gender variance form a very small subset of the data, hence we can club everyone with variability in their gender(neither male nor female) as gender_variance only for the sake of this analysis. There are values for Age that make no sense and so we restrict the Age column to be between 18 and 90 (and remove all rows that have nonsensical ages). Finally, we replace missing values in self_employed and work_interfere with “unknown”.\n How does seeking treatment look relative to the rest of the responses in the survey? Since many variables in the survey data are categorical, we can’t do a lot of ‘sexy’, numerical analysis. However, response counts (and proportions) can serve as a valuable variable in terms of insight.\nLet’s take the treatment variable for example, it corresponds to the question, Have you sought treatment for a mental health condition?:\nmental_health %\u0026gt;% count(treatment) ## # A tibble: 2 x 2 ## treatment n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 No 619 ## 2 Yes 632 It looks fairly balanced in terms of diversity in responses, no unknowns. We can look at differences in how employees who have been treated for mental health issues responded to some of the questions on the survey:\ntreatment \u0026lt;- mental_health %\u0026gt;% gather(Gender, self_employed, family_history, work_interfere, remote_work:obs_consequence, key = \u0026quot;question\u0026quot;, value = \u0026quot;response\u0026quot;) %\u0026gt;% select(question, response, treatment) %\u0026gt;% count(question, response, treatment) %\u0026gt;% spread(treatment, n) %\u0026gt;% mutate(total = No + Yes) treatment %\u0026gt;% arrange(-Yes/total) ## # A tibble: 60 x 5 ## question response No Yes total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 work_interfere Often 21 119 140 ## 2 Gender gender_variance 3 12 15 ## 3 work_interfere Sometimes 107 357 464 ## 4 family_history Yes 127 362 489 ## 5 work_interfere Rarely 51 122 173 ## 6 obs_consequence Yes 56 125 181 ## 7 care_options Yes 136 303 439 ## 8 Gender female 77 170 247 ## 9 leave Very difficult 31 66 97 ## 10 leave Somewhat difficult 44 81 125 ## # ... with 50 more rows This gives us proportions of treatment responses (Yes/No) within each response for each categorical question, we can now apply some statistical techniques to estimate what proportion an employee would say ‘Yes’ to the “Have you sought treatment for a mental health condition?” question.\nOne method that we can use is known as empirical bayes estimation. David Robinson gives an amazing introduction and explanation in his series about Empirical Bayes which starts with this post. We can treat the variable formed by dividing Yes by total, or the fraction of times “yes” is the response to the treatment question as the variable to estimate using the empirical bayes method. But first, let’s look at the distribution of Yes:\ntreatment %\u0026gt;% mutate(yes_prop = Yes/total) %\u0026gt;% ggplot(aes(yes_prop)) + geom_histogram(fill = \u0026quot;#bd1550\u0026quot;) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.8), family = \u0026quot;Merriweather\u0026quot;), plot.subtitle = element_text(size = rel(1.2), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,20,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0)), axis.text.x = element_text(size = rel(1.2)), plot.caption = element_text(margin = margin(10, 0, 0, 0)) ) + labs( title = \u0026quot;Distribution of proportions of employees \\nseeking treatment for mental health\u0026quot;, subtitle = \u0026quot;Within responses to other questions\\nin the OSMI mental health survey data\u0026quot;, x = \u0026quot;Proportion of employees who have sought treatment for mental health\u0026quot; ) Since this plot shows a probability distribution of rates, we can fit a beta distribution which takes evidence from the data as prior beliefs. All this can be done by the ebbr package which will use Bayes’ theorem to to get point estimates and 95% credible intervals for looking at the proportion of ‘Yes’ relative to all responses to the question.\ntreatment_estimate \u0026lt;- treatment %\u0026gt;% add_ebb_estimate(Yes, total) %\u0026gt;% select(question,response, Yes, total, .raw, .fitted, .low, .high) treatment_estimate ## # A tibble: 60 x 8 ## question response Yes total .raw .fitted .low .high ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 anonymity Don\u0026#39;t know 369 815 0.453 0.454 0.420 0.488 ## 2 anonymity No 37 64 0.578 0.569 0.456 0.678 ## 3 anonymity Yes 226 372 0.608 0.605 0.556 0.653 ## 4 benefits Don\u0026#39;t know 151 407 0.371 0.375 0.329 0.422 ## 5 benefits No 179 371 0.482 0.483 0.434 0.534 ## 6 benefits Yes 302 473 0.638 0.636 0.592 0.678 ## 7 care_options No 206 499 0.413 0.415 0.373 0.458 ## 8 care_options Not sure 123 313 0.393 0.397 0.345 0.451 ## 9 care_options Yes 303 439 0.690 0.686 0.642 0.728 ## 10 coworkers No 117 258 0.453 0.456 0.397 0.516 ## # ... with 50 more rows We can plot the confidence intervals along with the point estimate for each of the responses, this will result in a very long plot!\ntreatment_estimate %\u0026gt;% mutate(question = paste(question, response, sep = \u0026quot;: \u0026quot;)) %\u0026gt;% mutate(question = reorder(question, .fitted)) %\u0026gt;% filter(total \u0026gt; 100) %\u0026gt;% ggplot(aes(.fitted, question)) + geom_point(aes(size = total), color = \u0026quot;#8fbc94\u0026quot;) + geom_errorbarh(aes(xmin = .low, xmax = .high), color = \u0026quot;#8fbc94\u0026quot;) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(2.2), family = \u0026quot;Merriweather\u0026quot;, margin = margin(10, 0, 10, 0), hjust = 0), plot.subtitle = element_text(size = rel(1.5), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,30,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0), size = rel(1.3)), axis.text.x = element_text(size = rel(1.4)), plot.caption = element_text(margin = margin(10, 0, 0, 0), size = rel(1.2)), axis.title.x.top = element_text(margin = margin(0, 0, 20, 0)), axis.title.y = element_text(size = rel(1.3)), axis.text.y = element_text(size = rel(1.4)), legend.position = \u0026quot;top\u0026quot;, legend.title = element_text(size = rel(1.3)), legend.text = element_text(size = rel(1.1)) ) + scale_x_continuous(sec.axis = dup_axis(), labels = scales::percent_format(), limits = c(0, 0.9), breaks = seq(0, 0.9, by = 0.1)) + scale_size_continuous(range = c(2,6)) + labs ( title = \u0026quot;Responses of employees who have\\nsought treatment for mental health\u0026quot;, subtitle = \u0026quot;Based on responses of the OSMI mental health survey, 2014.\\nMinimum 100 employees in each response category.\\nIntervals are 95% credible.\u0026quot;, y = \u0026quot;Responses\u0026quot;, x = \u0026quot;people who sought treatment/total people in response category\u0026quot;, caption = \u0026quot;Source: Kaggle\u0026quot;, size = \u0026quot;Number of respondents\u0026quot; ) This plot shows all responses to questions and the proportion of respondents who have sought treatment in each response category. The different sizes of the points indicate the number of people who had that particular response as well as said ’Yes’to the treatment question.\nExample of interpretation: 83% of employees in the survey who felt their mental illness often interferes with their work (work_interfere addresses this question) have sought treatment for mental health. We can also see that many respondents with a family history (family_history) of mental illness as well as those who face difficulties in getting leave due to mental health issues (leave) have higher occurence in seeking treatment for mental health in the survey.\n Alternative visualization A better way to look at this same plot is separating it by questions so that identifying responses with higher amounts of treatment-seekers becomes more apparent. I have also parsed the full questions for each variable to show in this plot.\nquestions \u0026lt;- read_delim(\u0026quot;../../static/data/question_response.txt\u0026quot;, \u0026quot;:\u0026quot;) treatment_estimate %\u0026gt;% inner_join(questions, by = c(\u0026quot;question\u0026quot; = \u0026quot;question_var\u0026quot;)) %\u0026gt;% mutate(response = reorder(response, .fitted)) %\u0026gt;% filter(total \u0026gt; 100) %\u0026gt;% ggplot(aes(.fitted, response)) + geom_point(aes(size = total), color = \u0026quot;#8fbc94\u0026quot;) + geom_errorbarh(aes(xmin = .low, xmax = .high), color = \u0026quot;#8fbc94\u0026quot;) + facet_wrap(~question_text, scales = \u0026quot;free\u0026quot;, ncol = 2, labeller = labeller(question_text = label_wrap_gen(39))) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(2.2), family = \u0026quot;Merriweather\u0026quot;, margin = margin(10, 0, 10, 0), hjust = 0), plot.subtitle = element_text(size = rel(1.5), family = \u0026quot;Merriweather Light\u0026quot;, margin = margin(0,0,30,0)), text = element_text(family = \u0026quot;Noto Sans CJK JP Light\u0026quot;), axis.title.x = element_text(margin = margin(20, 0, 0, 0), size = rel(1.3)), axis.text.x = element_text(size = rel(1.4)), plot.caption = element_text(margin = margin(10, 0, 0, 0), size = rel(1.2)), axis.title.y = element_text(size = rel(1.3)), axis.text.y = element_text(size = rel(1.4)), legend.position = \u0026quot;top\u0026quot;, legend.title = element_text(size = rel(1.3)), legend.text = element_text(size = rel(1.1)), strip.text = element_text(size = rel(1.1), face = \u0026quot;bold\u0026quot;) ) + scale_x_continuous(labels = scales::percent_format(), limits = c(0, 1), breaks = seq(0, 1, by = 0.25)) + scale_size_continuous(range = c(2,5)) + labs ( title = \u0026quot;Responses of employees who have\\nsought treatment for mental health\u0026quot;, subtitle = \u0026quot;Based on responses of the OSMI mental health survey, 2014.\\nMinimum 100 employees in each response category.\\nIntervals are 95% credible.\u0026quot;, y = \u0026quot;Responses\u0026quot;, x = \u0026quot;people who sought treatment/total people in response category\u0026quot;, caption = \u0026quot;Source: Kaggle\u0026quot;, size = \u0026quot;Number of respondents\u0026quot; ) Adding the proper question texts as well as splitting the plot for each question really helps understand differences between each response in a given question, relative to the fraction of employees who have sought treatment. Most of the responses that result in higher peoportions of treatment seekers than their alternatives have some sort of an indication towards a negative consequence of work in regards to mental health. For example:\n respondents who feel their company does not take mental health as seriously as physical health also accounted for being high in number of treatment seekers. the same thing is observed with employees who think discussing their mental health issues with the employer can have a negative consequence   Conclusion Often times an employee with mental health issues will not seek treatment because they fear its effect on their work. In this analysis, we have seen how employees who seek treatment feel about their workplace in relation to their mental health and explored some of the differences in attitudes for other questions in the survey. Analyses similar to the one I have presented can contribute to, or potentially ignite further, much better research about mental health at workplaces. There is a lot more data than what I was able to present here and so there are a number of ideas that can be applied to this data, such as:\nCreating a disclosure metric that looks at the extent to which employees can dicuss mental issues with their employers (coworkers, supervisors, and during interview). This can be set up as an ordinal regression problem by making the metric ordinal, like a likert-scale (0-10). I have looked at seeking treatment and its proportions across different responses and response categories, the same can be done with any question with complete data (presence of missing variables causes more ambiguity than what is already present in the survey data due to sampling bias, both voluntary and no-response) Anything you can think of, the data is all present here along with the new, 2016 survey results which can be found here  I hope you enjoyed reading this analysis, it is my hope to continue writing more data-driven posts in the future as I have mentioned countless times in the past to be the major reason behind this website. I wish everyone a happy 2018!\nYou can check out the R code used to write this post here\n ","date":1514937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514937600,"objectID":"22f6c2e3800d8793da6c8f0607db89c2","permalink":"/post/osmi-mental-health-in-tech-survey-data/","publishdate":"2018-01-03T00:00:00Z","relpermalink":"/post/osmi-mental-health-in-tech-survey-data/","section":"post","summary":"Understanding and accepting mental health as an issue at the workplace has become ever so crucial in recent times. Mental illnesses like depression and anxiety can have a significant economic impact; the estimated cost to the global economy is US$ 1 trillion per year in lost productivity (source).","tags":["ggplot2","mental health","survey data"],"title":"Attitudes of employees towards mental health in the tech workplace","type":"post"},{"authors":null,"categories":[],"content":" Why? I feel no other content can do enough justice to a blog’s first post than the one describing why the author would take out time from his/her daily routine and write about stuff. So lets start with the why:\nOver the course of my undergraduate degree at Purdue, I have increasingly grown fond of the R programming language as part of my journey in a data driven career. The community surrounding the open-source language is one of the main reasons why I believe R has become so popular in data projects as well as data science teams in industry. Most of the active members of the R community are on twitter and their tweets or top quality blog posts keep inspiring me to push my limits as to what I can do with the power of data driven programming.\nDavid Robinson’s recent post serves as a great advice to all aspiring data scientists since it stresses on the importance of creating public artifacts and writing about them from a career perspective. David talks about how a data blog can help:\n Practicing skills you are proud of and showing them off by writing about them. Interacting with community by making work public and sharing. Learning about prospective skills to work on through community feedback.   As I hopefully transition into grad school, I’d like to stick to these principles while also putting emphasis on improving my communication of analysis and results which would more than just help me in my career.\n\u0026quot;Things that are still on your computer are approximately useless.\u0026quot; -@drob #eUSR #eUSR2017 pic.twitter.com/nS3IBiRHBn\n\u0026mdash; AmeliaMN (@AmeliaMN) November 3, 2017   What? While a blog about data science can certainly help my career, I am also curious to create content that focuses on trends and observations around the world. And so, a lot of (definitely not all) my posts would be around open data that describes conditions in a particular region, or the world as a whole. The major goal behind choosing this topic for a majority of my posts is to try and make the reader think about what they thought was going on and what is the actual, fact-based reality. As Daniel Kahneman puts it: we are bad intuitive statisticians\nSome posts will also focus on cultural analysis (like text analysis of song lyrics or a speech but not a tweet), while some will look at statistical and modeling concepts and some might just be random analyses that are supposed to make you laugh.\nBut one thing is for sure: none of my posts would be polished and I will not actually be change people’s opinions (contrary to the subtitle of the website) since that is more under their control, and I myself am still learning a lot about global devleopment. But I would still like to work on these problems in whatever small way I can and follow David Robinson’s mantra: sharing anything is almost always better than sharing nothing.\n How? And finally, the how. This would be rather short since it is much less important than what is produced as content.\nAs an R enthusiast, I am using the blogdown package and reference guide (written by Yihui Xie, Amber Thomas, Alison Presmanes Hill) to run this website. Blogdown integrates with Hugo to generate the posts on this website by rendering my analysis in Rmarkdown as markdown documents.\nAs an example:\nlibrary(gapminder) library(tidyverse) gapminder %\u0026gt;% filter(continent != \u0026quot;Oceania\u0026quot;) %\u0026gt;% ggplot(aes(year, lifeExp, group = country, color = country)) + geom_line(lwd = 1, show.legend = FALSE) + facet_wrap(~ continent) + scale_color_manual(values = country_colors) + scale_x_continuous(breaks = seq(1950, 2010, by = 10), limits = c(1950, 2010)) + scale_y_continuous(breaks = seq(20, 100, by = 20), limits = c(20, 85)) + theme_minimal() + theme( strip.text = element_text(size = rel(1.1), face = \u0026quot;bold\u0026quot;), plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;), plot.subtitle = element_text(hjust = 0.5, size = rel(1)) ) + ggtitle(\u0026quot;Life Expectancy of countries over the years (1952 - 2007)\u0026quot;) + labs( caption = \u0026quot;Source: Gapminder\u0026quot;, subtitle = \u0026quot;Countries in Oceania are ignored for some reason\u0026quot; ) And so there you have it, a small code snippet and a resulting plot - the essence of this blog along with the accompanying written content. If any of this sounds remotely interesting to you, I welcome you to kanishka.xyz!\n ","date":1513641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513641600,"objectID":"79f065ac261123b283374e014fca4177","permalink":"/post/first-post/","publishdate":"2017-12-19T00:00:00Z","relpermalink":"/post/first-post/","section":"post","summary":"Why? I feel no other content can do enough justice to a blog’s first post than the one describing why the author would take out time from his/her daily routine and write about stuff.","tags":[],"title":"The First Post","type":"post"},{"authors":null,"categories":null,"content":"\r  September 2020: A co-authored research article on enterprise risk complexity with Dr. Joe Sinfield is currently in early draft stage. It views enterprise risk management from a complexity science perspective and is intended for the journal Risk Analysis.\n  September 2020: A co-authored research article on pathways to enterprise resilience with Dr. Joe Sinfield is currently in late draft stage. It distills hitherto findings from my research, and is intended for executive redearship.\n  August 2020: Attended the Academy of Management\u0026rsquo;s paper development workshop on automated content analysis and participated in round-table discussions with brilliant academy mentors Pan Lingling and Tim Hannigan.\n  August 2020: Submitted a Doctoral Dissertation Research Improvement Grant (DDRIG) proposal to the National Science Foundation via Purdue University with Dr. Joe Sinfield and Dr. Julia Rayz as co- senior personnel.\n  August 2020: Paper with Dr. Joe Sinfield and Romika Kotian was accepted for publication in Sustainable Futures - Elsevier!\n  July 2020: Congratulations to Dr. Jucun Liu and Dr. Joe Sinfield of Purdue\u0026rsquo;s Innovation Science Laboratory for the acceptance of their paper on Business Models for publication in Long Range Planning!\n  July 2020: A co-authored research article on the role of SMEs in International Development with Romika Kotian and Dr. Joe Sinfield is currently in analysis stage.\n  July 2020: A co-authored research article on the systematic innovation in USA\u0026rsquo;s Marine Renewable Energy sector with Abhi Ajmani and Dr. Joe Sinfield is currently in early draft stage.\n  June 2020: Submitted three round 1 revisions in June!\n  May 2020: Wrote a blog post on the investment thesis of early-stage Venture Capital for University endowments.\n  ","date":1512104400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512104400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"/news/","publishdate":"2017-12-01T00:00:00-05:00","relpermalink":"/news/","section":"","summary":"\r\nList of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]