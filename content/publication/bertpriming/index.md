---
abstract: BERT is a language processing model trained for word prediction in context, which has shown impressive performance in natural language processing tasks. However, the principles underlying BERT's use of linguistic cues present in context are yet to be fully understood. In this work, we develop tests informed by the semantic priming paradigm to investigate BERT's handling of lexical relations to complete a cloze task (Taylor, 1953). We define priming to be an increase in BERT's expectation for a target word (pilot), in a context (e.g., I want to be a ___), when the context is prepended by a related word (airplane) as opposed to an unrelated one (table). We explore BERT's priming behavior under various predictive constraints placed on the blank, and find that BERT is sensitive to lexical priming effects only under minimal constraint from the input context. This pattern was found to be consistent across diverse lexical relations.
authors:
- admin 
- Allyson Ettinger
- Julia Taylor Rayz
date: "2020-08-01T00:00:00Z"
# doi: "10.1109/SMC.2019.8914528"
featured: true
image:
  # caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
  # focal_point: ""
  preview_only: false
links:
- name: Supplementary
  url: "papers/cogsci20supp.pdf"
publication: In *Proceedings of the 42nd Annual Conference of the Cognitive Science Society*
publication_short: In *CogSci 2020*
publication_types:
- "1"
publishDate: "2020-06-01T00:00:00Z"
# slides: example
summary: Using semantic priming to investigate how BERT utilizes lexical relations to inform word probabilities in context.
# tags:
# - Source Themes
title: Exploring Lexical Relations in BERT using Semantic Priming (Forthcoming)
# url_code: '#'
# url_dataset: '#'
url_pdf: "papers/cogsci20abstract.pdf"
url_poster: "posters/cogsci20.pdf"
# url_project: ""
# url_slides: "slides/fuzzy-risk.pdf"
# url_source: '#'
url_video: 'https://cutt.ly/bert-priming'
---

{{% alert note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /alert %}}

